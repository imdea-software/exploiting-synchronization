Programming languages for web applications, mobile devices and cloud
platforms provide constructs to allow programmers to expose as much
concurrency in their programs as possible. These features, designed to
improve performance and responsiveness,include asynchronous procedure
calls, passing tasks to and returning tasks from concurrent tasks to
procedures.  While providing programming ease and flexibility, the
combination of asynchronous programming features with shared memory
leads to interference between parts of a program that is hard for
programmers to predict.  As a result, bug detection tools for
asynchronous programs with shared memory are increasingly in
demand. One successful family of verification approaches is built
around the idea of parameterized schedulers and
sequentializations. Roughly speaking, the former refers to exploring
concurrent behaviors that are within a parameter $k$ of a default
concurrent execution, while the latter refers expressing this set of
behaviors as a sequential program. Sequentializations provide better coverage than dynamic tools by 
exploring program behavior for all inputs but keep complexity under control by careful choice of the parameter $k$. 

Let us informally refer to one possible concurrent behavior of such a program as a schedule, and let us refer to a module that selects what schedule to run a scheduler. 
One successful approach for analyzing such asynchronous programs is \emph{parameterized} schedulers -- schedulers that explore behaviors within a parameter $k$ of a default ``deterministic'' schedule. 
Sequentialization is a static verification approach that constructs a sequential program that encapsulates schedules within a parameter $k$ of a deterministic schedule, and uses SMT solvers to determine whether this set of executions contains an error. 


sequantialization -- 
constructing a sequential program that captures a set of concurrent behaviors of the original program close to a default concurrent behavior. 

Since
    asynchronous program analysis is rather challenging, current approaches
    sacrifice completeness and focus on a limited set of thread schedules that
    are empirically likely to expose programming errors. The consideration of
    effective \emph{parameterized} thread schedulers, which increasingly allow
    more diverse thread interactions at higher cost as their parameters are
    increased, is essential to such approaches. While the schedulers on which
    these analyses are based are designed to minimize the cost of exploration,
    they do not take into account synchronization between asynchronous tasks.
    
    In this work we hypothesize that the limited exploration of relevant
    asynchronous program behaviors can be made more efficient by designing
    schedulers which are sensitive to common synchronization mechanisms, such
    as waiting for an asynchronous task to complete. Accordingly, we design a
    reduction-based approach to analyzing asynchronous programs, which can
    leverage existing (sequential) program analysis tools, by encoding a
    limited set of thread schedules as executions of a sequential program; the
    volume of encoded thread schedules, and ultimately the exploration cost, is
    determined by a bounding parameter. By taking program synchronization into
    account, we are able to uncover programming errors at a lower analysis cost
    compared to synchronization-agnostic approaches; we validate this
    hypothesis both conceptually, by reasoning over classes of asynchronous
    programming errors, and empirically.


=================================================================

Roughly speaking, the former refers to exploring
concurrent behaviors that are within a parameter $k$ of a default
concurrent execution, while the latter refers expressing this set of
behaviors as a sequential program. Sequentializations provide better
coverage than dynamic tools by exploring program behavior for all
inputs but keep complexity under control by careful choice of the
parameter $k$. They thus sacrifice completeness and focus on a limited
set of thread schedules that are empirically likely to expose
programming errors. The consideration of effective
\emph{parameterized} thread schedulers, which increasingly allow more
diverse thread interactions at higher cost as their parameters are
increased, is essential to such approaches.