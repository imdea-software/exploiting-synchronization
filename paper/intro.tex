\section{Introduction}
\label{sec:intro}

\lstset{language=[Sharp]C}
\lstset{basicstyle=\ttfamily\scriptsize}

In order to improve program performance and responsiveness, many modern
programming languages and libraries promote an asynchronous programming model,
in which ``asynchronous procedures'' can execute concurrently with their
callers, until their callers explicitly wait for their completion. Accordingly,
as concurrently-executing procedures interleave their accesses to shared
memory, asynchronous programs are prone to concurrency-related errors.

In this work, we develop program analyses capable of detecting errors in
asynchronous programs. To motivate the need for such analyses, consider the
subtle error in the event-handling C\# code of a graphical user interface found
on StackOverflow, which is listed Figure~\ref{fig:onNav}.
The \lstinline{MySubClass.onNavigatedTo} method accesses image-related
information (\lstinline{m_bmp.PixelWidth}) which is filled in by the
\lstinline{LoadState} method, invoked by the \lstinline{OnNavigatedTo}
method of the base class. However, the \lstinline{LoadState} method has been
implemented to execute asynchronously so that its callers can continue to
execute while the image file is read --- which is presumably a high-latency
operation --- meaning that \lstinline{base.OnNavigatedTo} can return before
\lstinline{m_bmp} has been initialized. This creates a race between the
initialization of \lstinline{m_bmp} and its use in the call to
\lstinline{Canvas.SetLeft}, which results in an error when its use wins. Not
having anticipated this race, the programmer has failed to provide adequate
synchronization to ensure that the call to \lstinline{LoadState} completes
before \lstinline{m_bmp} is accessed by \lstinline{OnNavigatedTo}.

\begin{figure*}[t]
  \centering
  \begin{minipage}{14cm}
    \lstinputlisting{codes/overridenEx.cs}
  \end{minipage}
  \caption{This code contains a subtle bug due to a 
    race condition on the \protect\lstinline{m_bmp} field.}
  \label{fig:onNav}
\end{figure*}

While detecting such concurrency bugs by \emph{exhaustive} exploration of all
possible program schedules is intractable, one promising approach is the
\emph{prioritized} exploration of behaviors whose manifestations rely on a
small numbering of ordering dependencies between program operations. In
particular, the delay bounding approach~\cite{conf/popl/EmmiQR11} explores the
program behaviors arising in executions with a given scheduler~$S(K)$
parameterized by a ``delay bound''~$K \in \<Nats>$; while $S(0)$ is a
deterministic scheduler, exhibiting only one order of program operations,
$S(K)$ is given additional nondeterministic choice with each increasing value
of $K$, allowing additional orders, and ultimately, exhibiting additional
observable program behaviors. The approach is particularly compelling under the
hypothesis that interesting program behaviors (e.g.,~bugs) manifest with few
ordering dependencies: Emmi et al.~\cite{conf/popl/EmmiQR11} demonstrate an
efficiently-implementable ``depth-first'' delaying scheduler~$\df(K)$
which can expose behaviors with few ordering dependencies using small values of
$K$.

%THERE ARE PROGRAMS THAT DO NOT TERMINATE FOR FIXED K UNDER DF(K)

In practice, the cost of prioritized exploration with a parameterized scheduler
$S(K)$ is highly sensitive to the value of $K$, limiting $\df(K)$-based
exploration to roughly $0 \le K < 5$, depending on program size. While such
small values of $K$ may suffice to expose bugs in programs which use very
little synchronization, each program synchronization statement induces another
event-order dependency, possibly forcing $\df(K)$ to further deviate from its
natural deterministic order by increasing $K$. For instance, if $\df(K)$'s
default schedule encounters a statement which acquires a lock held by another
thread, then $\df(K)$ must spend one of its $K$ delays in order to execute the
other thread and eventually progress past the lock acquisition. In the context
of asynchronous programs, e.g.,~using C\#'s asynchronous methods, $\df(K)$ must
spend one if its $K$ delays to advance past a statement which waits for a
non-completed task to complete. It follows that program behaviors which can
appear only after a high number of synchronization statements carry a high
number of event-order dependencies, which ultimately may be exercised by
$\df(K)$ only for large values of $K$. As the cost of program exploration with
$\df(K)$ is sensitive to $K$, the discovery of such behaviors may
require an unreasonable amount of computing resources.

In this work we demonstrate a delaying scheduler $\dfw(K)$ for which the cost
of exploration is not tied to program synchronization, and yet which still
enjoys $\df(K)$'s strengths, in particular:
\begin{itemize}

  \item {\bf Sequentialization} The program executions allowed by $\dfw(K)$ can
    be simulated by a sequential program with nondeterministically-chosen
    data values.

  \item {\bf Low Complexity} The reachability problem for finite-data programs
  restricted to $\dfw(K)$ executions is NP-complete\footnote{This complexity
  assumes program variables are fixed in number and size, as
  usual~\cite{journals/fmsd/LalR09,conf/popl/EmmiQR11}.} in $K$.

\end{itemize}
However, unlike $\df(K)$, the $\dfw(K)$ scheduler explicitly takes program
synchronization into account in its scheduling decisions, so that event-order
dependencies arising from synchronization statements do not force $K$ to
increase. Effectively, this means that $\dfw(K)$ provides strictly greater
behavioral coverage than $\df(K)$ at virtually no additional cost.

Our contributions and outline are:
\begin{itemize}

  \item[\S\ref{sec:programs}] 
  A formal semantics of asynchronous programs with synchronization.

  \item[\S\ref{sec:schedulers}]
  A formal description of the $\dfw(K)$ scheduler, and comparison with $\df(K)$.
  
  \item[\S\ref{sec:comp}]
  A ``compositional semantics'' for $\dfw(K)$, which fosters sequentialization.
  
  \item[\S\ref{sec:seq}]
  A code translation encoding $\dfw(K)$ executions as a sequential program.
  
  \item[\S\ref{sec:complexity}]
  NP-completeness of reachability under $\dfw(K)$ for finite data programs.
  
  \item[\S\ref{sec:exp}]
  An empirical comparison: $\dfw(K)$ finds bugs faster, with smaller $K$.

\end{itemize}
In theory, every program behavior observable with $\df(K)$ is also observable
with $\dfw(K)$ for any $K$, yet the reverse is untrue: for any $K_0$ there exist
programs whose sets of behaviors observable with $\dfw(K_0)$ are not all
observable with $\df(K)$ for any $K$; Section~\ref{sec:schedulers} demonstrates
this fact. Empirically, Section~\ref{sec:exp} demonstrates that our
sequentialization of $\dfw(K)$ is more effective than $\df(K)$ in finding bugs
in real code examples as the number of synchronization operations grows.

While our development is centered around a simple programming model with
asynchronous procedure calls, and ``wait'' statements which block until the
completion of a given asynchronous call, our technical innovations also apply to other
asynchronous programming primitives provided by widely-used programming
languages, such as the partially-synchronous procedure calls of C\#\footnote{In
C\#, executing an ``await'' inside of a procedure returns control to the
caller, executing the remaining continuation asynchronously.} and the
wait-for-all synchronization barriers of, e.g., Cilk and X10. We believe that
the same principles would also apply for other synchronization mechanisms such
as semaphores and locks.
