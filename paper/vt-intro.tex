\section{Introduction}
\label{sec:intro}

\lstset{language=[Sharp]C}
\lstset{basicstyle=\ttfamily\scriptsize}

In order to improve performance and responsiveness, modern programming
languages for web applications, mobile devices and cloud platforms provide a
rich variety of asynchronous execution primitives. These primitives include
asynchronous procedure calls, asynchronous task handles being passed into and
out of procedures, task completions and event handlers.%unclear. reorder? Are asynchronous task handles being passed out of procedures, out of task completions and out of event handlers, or are there three separate things here 1) task handles being passed, 2) task completions, 3) event handlers? 
Even programs written
in single-threaded languages such as JavaScript can have non-deterministic
execution because of the order of execution of event handlers.

The flexibility provided by asynchronous programming primitives is very
valuable, but can appear deceptively simple. Languages such as C\# and
JavaScript also support shared variables, and the interplay between asynchrony
and shared variables can result in serious concurrency errors. To make matters
worse, the potential concurrency is not always clearly apparent in the program
syntax. We found numerous concurrency errors in asynchronous programs reported
in developer networks and discussion for a variety of programming languages and
execution programs. A typical example, taken from \ldots, is shown in
Figure~\ref{fig:onNav}.

\begin{figure}[t]
  \lstinputlisting{codes/overridenEx.cs}
  \caption{TODO: REVIEW CAPTION A bug example from StackOverflow \cite{} with subtle asynchrony?}
  \label{fig:onNav}
\end{figure}

In this example of graphical user interface event-handling code,
\lstinline{MySubClass}'s \lstinline{OnNavigatedTo} and 
\lstinline{LoadState} methods override the default
methods inherited from the \lstinline{LayoutAwarePage}
class. The programmer's intention is for the following two sequences of
events to execute in parallel 
\begin{enumerate}
\item \lstinline{MySubClass.OnNavigatedTo()} calls the base class's 
  \lstinline{LayoutAwarePage.OnNavigatedTo()} which invokes
  \lstinline{MySubClass.LoadState()}
\item \lstinline{MySubClass.OnNavigatedTo()} runs
  \lstinline{PlayIntroSoundAsync()} and completes
\end{enumerate}

(i) involves reading an image from a file. To provide a responsive
GUI, the programmer has made this method asynchronous and runs it
concurrently with (ii), with the implicit assumption that by the time
(ii) completes, (i) will have also completed.%clearer to just "(i) will have completed" or maybe "will also have completed"?
 However, since the
program has no synchronization to ensure that this will be the case,
some of the time, the last line of \lstinline{MySubClass.OnNavigatedTo()}
accesses image information before it is loaded, causing a crash. Since
the concurrent execution includes portions of sub and superclass
methods, and since the subclass overrides methods that were not
asynchronous with asynchronous ones, this error and ones of similar
nature are very hard to anticipate or detect by inspection. %"ones" seems to refer to two different things in this sentence. Rephrase.

%% TRANSITiON NEEDED

One particularly promising approach to asynchronous program analysis is the
prioritized exploration of program behaviors whose manifestations rely on a
small number of ordering dependencies between program events. In particular,
the delay bounding approach~\cite{conf/popl/EmmiQR11} explores the program
behaviors arising in executions with a given event scheduler~$S(k)$
parameterized by a ``delay bound''~$k \in \<Nats>$; while $S(0)$ is a
deterministic scheduler, allowing only one event order,%either this comma or the previous one should probably be removed (unless I am misunderstanding the sentence)
 with each increasing
value of $k$, $S(k)$ is given additional nondeterministic choice, allowing
additional event orders, and%,
 ultimately, exhibiting additional observable
program behaviors. The%This approach
 approach is particularly compelling under the hypothesis
that interesting program behaviors (e.g.,~bugs) manifest with few event-order
dependencies: Emmi et al.~\cite{conf/popl/EmmiQR11} demonstrate a parameterized
``depth-first'' delaying scheduler~$\mathrm{DF}(k)$ under which program
exploration can be carried out efficiently for small, fixed $k$, and for which
behaviors manifesting with few event-order dependencies are expressed with
small $k$.

However, in practice, event-order dependencies arise not only as concise
descriptors for sets of executions leading to certain program behaviors;%this is awkward. Read this independent clause as a full sentence. "not only as" sounds weird.
event-order dependencies also arise from the program text, in the form of
program synchronization. %this clause also sounds weird on its own and should probably be rephrased. You should also probably use a colon rather than a semi-colon since the second clause adds explanation/detail to the first.
For instance, a program statement which%that (restrictive clause)
 acquires a
lock cannot be executed until the lock is released. In the context of %Can you just say "in asynchronous programs?"
asynchronous programs, e.g.,~using C\#'s asynchronous methods, a statement
which %that (or "waiting") 
waits for a task to complete cannot be executed until that task's
procedure returns a result. It follows that program behaviors which%that (particularly bc which is used correctly later in this sentence
 can appear
only after a high number of synchronization statements carry a high number of
event-order dependencies, which ultimately may be exercised by the
$\mathrm{DF}(k)$ scheduler only for large values of $k$. Intuitively, each
additional event-order dependency may entail additional deviations from
$\mathrm{DF}(k)$'s natural deterministic order, forcing increased flexibility
via additional nondeterministic choices, i.e.,~increased $k$. Since the cost of
program exploration with $\mathrm{DF}(k)$ is highly sensitive to $k$, this%What is "this" here? If "this" is the sensitivity to k, omit "this means that" and begin the next clause with "the discovery..."
means that the discovery of such behaviors may require an unreasonable amount
of computing resources.

In this work we demonstrate a new delaying scheduler $\mathrm{DFW}(k)$ which%that
provides greater behavioral coverage than $\mathrm{DF}(k)$ with virtually no
additional cost. Unlike $\mathrm{DF}(k)$, the $\mathrm{DFW}(k)$ scheduler
interprets program synchronization so that the value of $k$ necessary to
exhibit any given program behavior does not depend on the number of event-order
dependencies arising from synchronization statements. Furthermore,
$\mathrm{DFW}(k)$ exhibits the following properties:
\begin{itemize}

  \item $\mathrm{DFW}(k)$ provides strictly greater coverage than
  $\mathrm{DF}(k)$: while the observable behaviors of any program with
  $\mathrm{DF}(k)$ are included in those of $\mathrm{DFW}(k)$ for any $k \in
  \<Nats>$, there exist programs for which the behaviors allowed by
  $\mathrm{DFW}(k_0)$, for some fixed $k_0$, are not included in
  $\mathrm{DF}(k)$ for any $k \in \<Nats>$.
    
  \item The cost of exploration with $\mathrm{DFW}(k)$ is no greater than that
  with $\mathrm{DF}(k)$: theoretically, the state-reachability problem for
  programs with finite data domains is NP-complete for both $\mathrm{DFW}(k)$
  and $\mathrm{DF}(k)$, for any fixed $k$. Empirically, we find that
  exploration with $\mathrm{DFW}(k)$ finds programming errors faster, and with
  fewer delays, than with $\mathrm{DF}(k)$.

  \item As with $\mathrm{DF}(k)$, program reachability with the
  $\mathrm{DFW}(k)$ scheduler succinctly reduces to sequential program
  reachability, enabling efficient delay-bounded asynchronous program analysis
  using existing sequential program analysis tools.

\end{itemize}

While our development of $\mathrm{DFW}(k)$ is centered around a simple
programming model with asynchronous procedure calls, and ``wait'' statements
which%that
 block until the completion of a given async call, our technical
innovations also apply for other variations, including the
partially-synchronous procedure calls of C\#\footnote{In C\#, executing a
``wait'' inside of a procedure returns control to the caller, executing the
remaining continuation asynchronously.} and the wait-for-all synchronization
barriers of, e.g., Cilk and X10. While we believe that the same principles
would also apply for other synchronization mechanisms such as semaphores and
locks, language-based mechanisms such as ``wait'' statements are ultimately
easier to exploit.
