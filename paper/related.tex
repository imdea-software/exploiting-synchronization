\section{Related Work}
\label{sec:related}

Our work follows the line of research on compositional reductions from
concurrent to sequential programs. The initial so-called
``sequentialization''~\cite{conf/pldi/QadeerW04} explored multi-threaded
programs up to one context-switch between threads, and was later expanded to
handle a parameterized amount of context-switches between a
statically-determined set of threads executing in round-robin
order~\cite{conf/tacas/QadeerR05,journals/fmsd/LalR09}. La~Torre et
al.~\cite{conf/cav/TorreMP10} later extended the approach to handle programs
parameterized by an unbounded number of statically-determined threads, and
shortly after, Emmi et al.~\cite{conf/popl/EmmiQR11} further extended these
results to handle an unbounded amount of dynamically-created tasks, which
besides applying to multi-threaded programs, naturally handles asynchronous
event-driven programs~\cite{conf/cav/SenV06}. Bouajjani et
al.~\cite{conf/sas/BouajjaniEP11} pushed these results even further to a
sequentialization which attempts to explore as many behaviors as possible
within a given analysis budget. While others have continued to propose
sequentializations for other bounded concurrent exploration criteria or program
models~\cite{conf/spin/KiddJV10,conf/tacas/GargM11,conf/tacas/BouajjaniE12,conf/cav/AtigBEL12,conf/sas/EmmiL12,conf/fse/EmmiLQ12},
as far as we are aware, none of these sequentializations are based on a
parameterized scheduler which can reduce exploration cost by taking into
account program synchronization.

While Emmi et al.'s work~\cite{conf/popl/EmmiQR11} is indeed the starting point
for our work, and the syntactic difference between our sequentializations is
small, we believe our contribution is significant for the following reasons:

First, and more technically, besides the statements appearing in the
translation of the wait statement, our DFW sequentialization must generalize
DF. Our translation must repeatedly make guesses --- once at each encountered
wait statement --- for the global state at which begins the sequence of
asynchronous tasks called until the next-encountered wait statement (which must
be equal to the global state reached by the next-encountered wait statement).
In the case of DF, the global state at which the sequence of \emph{all}
asynchronous tasks begin is fixed once and for all (and must be equal to the
global state reached by main). This extension is subtle, yet crucial.

Second, it is challenging to design a translation which
\begin{itemize}

  \item[(A)] correctly preserves causal information flow in the original
  program, while

  \item[(B)] ensuring that the concurrent executions simulated by our
  sequential program never block because of wait statements.

\end{itemize}
While the relatively ``easy'' alternative translation listed in
Figure~\ref{fig:tr:wait} does satisfy~A, it fails to satisfy~B. Our formal
development of the DFW sequentialization is a principled way to design a
translation which satisfies both Properties~A and~B: we show that the
executions admitted by the DFW scheduler (satisfying~B) coincide exactly with
our compositional semantics (satisfying~A), bridging the gap between any given
asynchronous program and its sequentialization.

Finally, comparing to approaches based on dynamic program exploration, while
delay-bounding using techniques such as Chess~\cite{conf/pldi/MusuvathiQ07}
could capture the same sets of concurrent interleavings for a given delay
bound, our static approach promises higher coverage: by using SMT-based
symbolic reasoning engines, we can reason about many possible program data
values at once, whereas dynamic techniques consider single concrete values. In
practice this could allow us to catch data-dependent bugs undetected by a given
dynamic technique.
